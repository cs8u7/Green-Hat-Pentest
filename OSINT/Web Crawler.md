![](image/Pasted%20image%2020241001101355.png)

- The web spiderbot prepares a list of URLs (Seed URLs) as input for the crawl process. 
- The selection of seed URLs depends on the goals of the web crawler. For each endpoint in the seed list, the crawler uses HTTP requests to fetch the HTML content of the web pages and parses it based on HTML tags. 
- Then, the crawler extracts useful information, most of which are external links pointing to new websites.
- Sometimes, the crawler can detect sensitive data in the HTML content. In the view of Googlebot, for searching purposes, it prioritizes extracting content in header tags and images. After finishing the downloading process of the seed list, the crawler removes duplicates, irrelevant links, or URLs that already exist in the seed list. 
- URLs are prioritized based on the crawler's algorithms, considering factors like page importance and update frequency while scheduling to avoid server overload. 
- Unlike some specific-purpose spider bots, crawlers that serve search engines like Google or Bing index URLs based on their rank (number of backlinks) and categories to enhance performance. 
- backlinks also known as inbound links or incoming links, are links from one website to a page on another website. It is an external link placed on other sites that point to your website. 
- Last, the crawler updates new-found URLs into the seed list to initialize new crawl processes.